{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMxL4v2U9Wrhte5auagmfmS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"in0PyicHhZDG"},"outputs":[],"source":["import datetime\n","\n","from google.colab import drive\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"73ieMA485Tme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666029803136,"user_tz":-180,"elapsed":2675,"user":{"displayName":"Boris Zhestkov","userId":"15589718157134474454"}},"outputId":"8fc3c358-2697-4ea5-c4f2-eba29dc16ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["data_dir = 'drive/My Drive/'\n","train_lang = 'en'"],"metadata":{"id":"Os4tVkvmkTIp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DatasetSeq(Dataset):\n","    def __init__(self, data_dir, train_lang='en'):\n","\t#open file\n","        with open(data_dir + train_lang + '.train', 'r') as f:\n","            train = f.read().split('\\n\\n')\n","\n","        # delete extra tag markup\n","        train = [x for x in train if not '_ ' in x]\n","\t    #init vocabs of tokens for encoding {<str> token: <int> id}\n","        self.target_vocab = {} # {p: 1, a: 2, r: 3, pu: 4}\n","        self.word_vocab = {} # {cat: 1, sat: 2, on: 3, mat: 4, '.': 5}\n","        self.char_vocab = {} # {c: 1, a: 2, t: 3, ' ': 4, s: 5}\n","\t    \n","        # Cat sat on mat. -> [1, 2, 3, 4, 5]\n","        # p    a  r  p pu -> [1, 2, 3, 1, 4]\n","        # chars  -> [1, 2, 3, 4, 5, 2, 3, 4]\n","\n","\t    #init encoded sequences lists (processed data)\n","        self.encoded_sequences = []\n","        self.encoded_targets = []\n","        self.encoded_char_sequences = []\n","        # n=1 because first value is padding\n","        n_word = 1\n","        n_target = 1\n","        n_char = 1\n","        for line in train:\n","            sequence = []\n","            target = []\n","            chars = []\n","            for item in line.split('\\n'):\n","                if item != '':\n","                    word, label = item.split(' ')\n","\n","                    if self.word_vocab.get(word) is None:\n","                        self.word_vocab[word] = n_word\n","                        n_word += 1\n","                    if self.target_vocab.get(label) is None:\n","                        self.target_vocab[label] = n_target\n","                        n_target += 1\n","                    for char in word:\n","                        if self.char_vocab.get(char) is None:\n","                            self.char_vocab[char] = n_char\n","                            n_char += 1\n","                    sequence.append(self.word_vocab[word])\n","                    target.append(self.target_vocab[label])\n","                    chars.append([self.char_vocab[char] for char in word])\n","            self.encoded_sequences.append(sequence)\n","            self.encoded_targets.append(target)\n","            self.encoded_char_sequences.append(chars)\n","\n","    def __len__(self):\n","        return len(self.encoded_sequences)\n","\n","    def __getitem__(self, index):\n","        return {\n","            'data': self.encoded_sequences[index], # [1, 2, 3, 4, 6] len=5\n","            'char': self.encoded_char_sequences[index],# [[1,2,3], [4,5], [1,2], [2,6,5,4], []] len=5\n","            'target': self.encoded_targets[index], #  (1)\n","        }"],"metadata":{"id":"SI8UCZuy7hTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = DatasetSeq(data_dir)"],"metadata":{"id":"dhJuBtoz7f43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#padding\n","# seq1 = [1, 2, 3, 4]\n","# seq2 = [9, 7, 6, 4, 3, 7, 5]\n","# pad seq1 equal seq2\n","# seq1 = [1, 2, 3, 4, 0, 0, 0]\n","# concat(seq1, seq2) [[1, 2, 3, 4, 0, 0, 0],\n","#                     [9, 7, 6, 4, 3, 7, 5]]"],"metadata":{"id":"0zXXXYP37gFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(input_data):\n","    data = []\n","    chars = []\n","    targets = []\n","    max_len = 0\n","    for item in input_data:\n","        if len(item['data']) > max_len:\n","            max_len = len(item['data'])\n","        data.append(torch.as_tensor(item['data']))\n","        chars.append(item['char'])\n","        targets.append(torch.as_tensor(item['target']))\n","    chars_seq = [[torch.as_tensor([0]) for _ in range(len(input_data))] for _ in range(max_len)]\n","    for j in range(len(input_data)):\n","        for i in range(max_len):\n","            if len(chars[j]) > i:\n","                chars_seq[i][j] = torch.as_tensor(chars[j][i])\n","    for j in range(max_len):\n","        chars_seq[j] = pad_sequence(chars_seq[j], batch_first=True, padding_value=0)\n","    data = pad_sequence(data, batch_first=True, padding_value=0)\n","    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n","    return {'data': data, 'chars': chars_seq, 'target': targets}"],"metadata":{"id":"uPJauY4hAqJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CharRNN(nn.Module):\n","    def __init__(self, vocab_size, emb_dim, hidden_dim):\n","        super().__init__()\n","        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n","        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n","\n","    def forward(self, x):\n","        emb = self.char_emb(x)\n","        _, out = self.gru(emb) # 1 x B x Hid\n","\n","        return out.squeeze().unsqueeze(1) # B x 1 x Hid    "],"metadata":{"id":"KTz2txO4LTZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNNPredictorV2(nn.Module):\n","    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes, \n","                 char_vocab, char_emb, char_hidden, ):\n","        super().__init__()\n","        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n","        #TODO try to use other RNN archicetures, f.e. RNN and LSTM\n","        self.gru = nn.GRU(emb_dim + char_hidden, hidden_dim, batch_first=True)\n","        self.classifier = nn.Linear(hidden_dim, n_classes)\n","        self.hidden_dim = hidden_dim\n","        self.char_rnn = CharRNN(char_vocab, char_emb, char_hidden)\n","    \n","    def forward(self, x, chars):\n","        emb = self.word_emb(x)\n","        char_features = [self.char_rnn(c.to(x.device)) for c in chars]\n","        char_features = torch.cat(char_features, dim=1)\n","        emb = torch.cat((emb, char_features), dim=-1)\n","        hidden, _ = self.gru(emb)\n","\n","        classes = self.classifier(hidden)\n","\n","        return classes"],"metadata":{"id":"WBFZc1qY6HsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hyper params\n","vocab_size = len(dataset.word_vocab) + 1\n","n_classes = len(dataset.target_vocab) + 1\n","n_chars = len(dataset.char_vocab) + 1\n","#TODO try to use other model parameters\n","emb_dim = 256\n","hidden = 256\n","char_hid = 64\n","char_emb = 32\n","n_epochs = 10\n","batch_size = 64\n","cuda_device = 0\n","batch_size = 100\n","device = f'cuda:{cuda_device}' if cuda_device != -1 else 'cpu'"],"metadata":{"id":"K_PACmDaH8Z7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RNNPredictorV2(vocab_size, emb_dim, hidden, n_classes, n_chars, char_emb, char_hid).to(device)\n","model.train()\n","optim = torch.optim.Adam(model.parameters(), lr=0.001)\n","loss_func = nn.CrossEntropyLoss()"],"metadata":{"id":"a4gX5zVDIZdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jVX0P0otIk4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for epoch in range(n_epochs):\n","    dataloader = DataLoader(dataset, \n","                            batch_size, \n","                            shuffle=True, \n","                            collate_fn=collate_fn,\n","                            drop_last = True,\n","                            )\n","    for i, batch in enumerate(dataloader):\n","        optim.zero_grad()\n","\n","        predict = model(batch['data'].to(device), batch['chars'])\n","        loss = loss_func(predict.view(-1, n_classes),\n","                         batch['target'].to(device).view(-1), \n","                         )\n","        loss.backward()\n","        optim.step()\n","        if i % 100 == 0:\n","            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n","   \n","    torch.save(model.state_dict(), f'./rnn_chkpt_{epoch}.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2f3MATJ8GKb","executionInfo":{"status":"ok","timestamp":1666030060798,"user_tz":-180,"elapsed":219700,"user":{"displayName":"Boris Zhestkov","userId":"15589718157134474454"}},"outputId":"4d1707bc-9234-44a4-b3c3-a4774a9abd5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, step: 0, loss: 2.9583592414855957\n","epoch: 0, step: 100, loss: 0.2921583950519562\n","epoch: 0, step: 200, loss: 0.1505596786737442\n","epoch: 1, step: 0, loss: 0.16022281348705292\n","epoch: 1, step: 100, loss: 0.07004785537719727\n","epoch: 1, step: 200, loss: 0.1104632318019867\n","epoch: 2, step: 0, loss: 0.08514568209648132\n","epoch: 2, step: 100, loss: 0.06940208375453949\n","epoch: 2, step: 200, loss: 0.021149974316358566\n","epoch: 3, step: 0, loss: 0.05598089098930359\n","epoch: 3, step: 100, loss: 0.06506560742855072\n","epoch: 3, step: 200, loss: 0.0708589032292366\n","epoch: 4, step: 0, loss: 0.05820649117231369\n","epoch: 4, step: 100, loss: 0.06536628305912018\n","epoch: 4, step: 200, loss: 0.055076759308576584\n","epoch: 5, step: 0, loss: 0.05656067654490471\n","epoch: 5, step: 100, loss: 0.04058987647294998\n","epoch: 5, step: 200, loss: 0.04071635380387306\n","epoch: 6, step: 0, loss: 0.03484119102358818\n","epoch: 6, step: 100, loss: 0.04943253844976425\n","epoch: 6, step: 200, loss: 0.039050742983818054\n","epoch: 7, step: 0, loss: 0.03108307160437107\n","epoch: 7, step: 100, loss: 0.03716171160340309\n","epoch: 7, step: 200, loss: 0.03939993679523468\n","epoch: 8, step: 0, loss: 0.0272400863468647\n","epoch: 8, step: 100, loss: 0.023144541308283806\n","epoch: 8, step: 200, loss: 0.023673400282859802\n","epoch: 9, step: 0, loss: 0.019285432994365692\n","epoch: 9, step: 100, loss: 0.01943451538681984\n","epoch: 9, step: 200, loss: 0.020000535994768143\n"]}]},{"cell_type":"code","source":["#example\n","#TODO modify inference for model with char input\n","phrase = 'He ran quickly after the red bus and caught it'\n","words = phrase.split(' ')\n","tokens = [dataset.word_vocab[w] for w in words]\n","\n","start = datetime.datetime.now()\n","with torch.no_grad():\n","    model.eval()\n","    predict = model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n","    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n","    end = datetime.datetime.now() - start\n","\n","target_labels = list(dataset.target_vocab.keys())\n","print([target_labels[l-1] for l in labels])"],"metadata":{"id":"9CljFAzIMMEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666028388211,"user_tz":-180,"elapsed":573,"user":{"displayName":"Boris Zhestkov","userId":"15589718157134474454"}},"outputId":"ac850afb-506a-4938-fd9c-4f92fb1fbb9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['PRON', 'VERB', 'ADV', 'SCONJ', 'DET', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'PRON']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"soes4kIU8FDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9PbgCjN48FRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"74gggSX58Fe9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"-57Jq-CW8NmD"}}]}