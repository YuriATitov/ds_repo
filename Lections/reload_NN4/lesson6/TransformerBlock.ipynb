{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMUt0uPerCPp8mMKuZXewvO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HAytw8yW-6hb"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np"]},{"cell_type":"code","source":["class ScaledDotProductAttention(nn.Module):\n","    ''' Scaled Dot-Product Attention '''\n","\n","    def __init__(self, temperature=1, attn_dropout=0.1):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.dropout = nn.Dropout(attn_dropout)\n","        self.softmax = nn.Softmax(dim=2)\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        attn = torch.bmm(q, # B x T1 x V\n","                         k.transpose(1, 2), # B x T2 x V -> B x V x T2\n","                         ) # B x T1 x T2\n","        attn = attn / self.temperature\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(~mask, -np.inf)\n","\n","        attn = self.softmax(attn)\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(~mask, 0.)\n","\n","        attn = self.dropout(attn)\n","        output = torch.bmm(attn, v) # B x T1 x T2 @ B x T1 x V\n","\n","        return output, attn"],"metadata":{"id":"WGJ99bTM_Id4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sinusoid_encoding_table(n_position, d_hid, padding_idx):\n","    ''' Sinusoid position encoding table '''\n","\n","    def cal_angle(position, hid_idx):\n","        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n","\n","    def get_posi_angle_vec(position):\n","        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n","\n","    sinusoid_table = np.array([get_posi_angle_vec(pos_i)\n","                               for pos_i in range(n_position)])\n","\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n","\n","    #change is None term on >= 0\n","    if padding_idx >= 0:\n","        # zero vector for padding dimension\n","        sinusoid_table[padding_idx] = 0.\n","\n","    return torch.FloatTensor(sinusoid_table)"],"metadata":{"id":"MpZRF97y_TSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, input_dim: int, head_dim: int, n_heads: int, emb_dim: int):\n","        super().__init__()\n","        self.head_dim = head_dim\n","        self.n_heads = n_heads\n","        # self.w_key = [nn.Linear(input_dim, head_dim, bias=False) for _ in range(n_heads)]\n","        self.w_key = nn.Linear(input_dim, head_dim * n_heads, bias=False)\n","        self.w_query = nn.Linear(input_dim, head_dim * n_heads, bias=False)\n","        self.w_value = nn.Linear(input_dim, head_dim * n_heads, bias=False)\n","\n","        self.attn = ScaledDotProductAttention(temperature=np.power(head_dim, 0.5))\n","\n","        self.proj = nn.Linear(n_heads * head_dim, emb_dim, bias=False)\n","\n","    def forward(self, k, q, v):\n","        b, t, _ = k.size()\n","        k = self.w_key(k)\n","        q = self.w_query(q)\n","        v = self.w_value(v)\n","\n","        # B x T x n_heads* head_dim -> B x T x n_heads x head_dim\n","        k = k.view(b, t, self.n_heads, self.head_dim)\n","        q = q.view(b, t, self.n_heads, self.head_dim)\n","        v = v.view(b, t, self.n_heads, self.head_dim)\n","\n","        # B x T x n_heads x head_dim -> n_heads x B x T x head_dim -> n_heads*B x T x head_dim\n","        k = k.permute(2, 0, 1, 3).contigious().view(-1, t, self.head_dim)\n","        q = q.permute(2, 0, 1, 3).contigious().view(-1, t, self.head_dim)\n","        v = v.permute(2, 0, 1, 3).contigious().view(-1, t, self.head_dim)\n","\n","        output, att = self.attn(q, k, v)\n","\n","        output = output.view(self.n_heads, b, t, self.head_dim).permute(\n","            1, 2, 0, 3).contigious().view(b, t, -1)\n","        \n","        output = self.proj(output)\n","\n","        return output"],"metadata":{"id":"jZDP5AYq_r-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FFNet(nn.Module):\n","    def __init__(self,input_dim: int, hidden_dim: int):\n","        super().__init__()\n","        self.linear1= nn.Linear(input_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, input_dim)\n","\n","        self.activ = nn.ReLU()\n","        self.do = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        return self.linear2(self.do(self.activ(self.linear1(x))))\n","        "],"metadata":{"id":"Da6kr_AFF6ts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FFTBlock(nn.Module):\n","    def __init(self, input_dim, head_dim, n_heads, hidden_dim):\n","        super().__init__()\n","        self.multiheadattn = MultiHeadAttention(input_dim, head_dim, n_heads, input_dim)\n","        self.ffnet = FFNet(input_dim, hidden_dim)\n","        self.ln1 = nn.LinearNorm()\n","        self.ln2 = nn.LinearNorm()\n","\n","    def forward(self, x):\n","        residual = x\n","        x = self.multiheadattn(x, x ,x)\n","        x = self.ln1(residual + x)\n","\n","        residual = x\n","        x = self.ffnet(x)\n","        x = self.ln2(residual + x)\n","\n","        return x"],"metadata":{"id":"6XN8vVepG5pq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v0H8dfnIH-4O"},"execution_count":null,"outputs":[]}]}