{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOVPV6Asb6JT/YoFr4ETX8m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"in0PyicHhZDG"},"outputs":[],"source":["import datetime\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"73ieMA485Tme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666028019672,"user_tz":-180,"elapsed":23150,"user":{"displayName":"Boris Zhestkov","userId":"15589718157134474454"}},"outputId":"a3f77f6e-95ee-45ec-ae26-054547589686"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_dir = 'drive/My Drive/'\n","train_lang = 'en'"],"metadata":{"id":"Os4tVkvmkTIp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class DatasetSeq(Dataset):\n","    def __init__(self, data_dir, train_lang='en'):\n","\t#open file\n","        with open(data_dir + train_lang + '.train', 'r') as f:\n","            train = f.read().split('\\n\\n')\n","\n","        # delete extra tag markup\n","        train = [x for x in train if not '_ ' in x]\n","\t    #init vocabs of tokens for encoding {<str> token: <int> id}\n","        self.target_vocab = {} # {p: 1, a: 2, r: 3, pu: 4}\n","        self.word_vocab = {} # {cat: 1, sat: 2, on: 3, mat: 4, '.': 5}\n","        self.char_vocab = {} # {c: 1, a: 2, t: 3, ' ': 4, s: 5}\n","\t    \n","        # Cat sat on mat. -> [1, 2, 3, 4, 5]\n","        # p    a  r  p pu -> [1, 2, 3, 1, 4]\n","        # chars  -> [1, 2, 3, 4, 5, 2, 3, 4]\n","\n","\t    #init encoded sequences lists (processed data)\n","        self.encoded_sequences = []\n","        self.encoded_targets = []\n","        self.encoded_char_sequences = []\n","        # n=1 because first value is padding\n","        n_word = 1\n","        n_target = 1\n","        n_char = 1\n","        for line in train:\n","            sequence = []\n","            target = []\n","            chars = []\n","            for item in line.split('\\n'):\n","                if item != '':\n","                    word, label = item.split(' ')\n","\n","                    if self.word_vocab.get(word) is None:\n","                        self.word_vocab[word] = n_word\n","                        n_word += 1\n","                    if self.target_vocab.get(label) is None:\n","                        self.target_vocab[label] = n_target\n","                        n_target += 1\n","                    for char in word:\n","                        if self.char_vocab.get(char) is None:\n","                            self.char_vocab[char] = n_char\n","                            n_char += 1\n","                    sequence.append(self.word_vocab[word])\n","                    target.append(self.target_vocab[label])\n","                    chars.append([self.char_vocab[char] for char in word])\n","            self.encoded_sequences.append(sequence)\n","            self.encoded_targets.append(target)\n","            self.encoded_char_sequences.append(chars)\n","\n","    def __len__(self):\n","        return len(self.encoded_sequences)\n","\n","    def __getitem__(self, index):\n","        return {\n","            'data': self.encoded_sequences[index], # [1, 2, 3, 4, 6] len=5\n","            'char': self.encoded_char_sequences[index],# [[1,2,3], [4,5], [1,2], [2,6,5,4], []] len=5\n","            'target': self.encoded_targets[index], #  (1)\n","        }"],"metadata":{"id":"SI8UCZuy7hTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = DatasetSeq(data_dir)"],"metadata":{"id":"dhJuBtoz7f43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#padding\n","# seq1 = [1, 2, 3, 4]\n","# seq2 = [9, 7, 6, 4, 3, 7, 5]\n","# pad seq1 equal seq2\n","# seq1 = [1, 2, 3, 4, 0, 0, 0]\n","# concat(seq1, seq2) [[1, 2, 3, 4, 0, 0, 0],\n","#                     [9, 7, 6, 4, 3, 7, 5]]"],"metadata":{"id":"0zXXXYP37gFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    data = []\n","    target = []\n","    for item in batch:\n","        data.append(torch.as_tensor(item['data']))\n","        target.append(torch.as_tensor(item['target']))\n","    data = pad_sequence(data, batch_first=True, padding_value=0)\n","    target = pad_sequence(target, batch_first=True, padding_value=0)\n","\n","    return {'data': data, 'target': target}"],"metadata":{"id":"uPJauY4hAqJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNNPredictor(nn.Module):\n","    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n","        super().__init__()\n","        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n","        self.gru_cell = nn.GRUCell(emb_dim, hidden_dim)\n","        self.classifier = nn.Linear(hidden_dim, n_classes)\n","        self.hidden_dim = hidden_dim\n","\n","    def forward(self, x): # B x T\n","        b, t = x.size()\n","        emb = self.word_emb(x) # B x T x Emb\n","        gru_out = []\n","        hidden = torch.zeros(b, self.hidden_dim).to(emb.device)\n","        for i in range(t):\n","            hidden = self.gru_cell(emb[:, i, :], hidden)\n","            gru_out.append(hidden.unsqueeze(1))\n","        gru_out = torch.cat(gru_out, dim=1)\n","\n","        classes = self.classifier(gru_out)\n","\n","        return gru_out\n"],"metadata":{"id":"KTz2txO4LTZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNNPredictorV2(nn.Module):\n","    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n","        super().__init__()\n","        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n","        #TODO try to use other RNN archicetures, f.e. RNN and LSTM\n","\n","        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n","        self.classifier = nn.Linear(hidden_dim, n_classes)\n","        self.hidden_dim = hidden_dim\n","    \n","    def forward(self, x):\n","        emb = self.word_emb(x)\n","        hidden, _ = self.gru(emb)\n","\n","        classes = self.classifier(hidden)\n","\n","        return classes"],"metadata":{"id":"WBFZc1qY6HsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hyper params\n","vocab_size = len(dataset.word_vocab) + 1\n","n_classes = len(dataset.target_vocab) + 1\n","n_chars = len(dataset.char_vocab) + 1\n","#TODO try to use other model parameters\n","emb_dim = 256\n","hidden = 256\n","n_epochs = 10\n","batch_size = 64\n","cuda_device = 0\n","batch_size = 100\n","device = f'cuda:{cuda_device}' if cuda_device != -1 else 'cpu'"],"metadata":{"id":"K_PACmDaH8Z7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RNNPredictorV2(vocab_size, emb_dim, hidden, n_classes).to(device)\n","model.train()\n","optim = torch.optim.Adam(model.parameters(), lr=0.001)\n","loss_func = nn.CrossEntropyLoss()"],"metadata":{"id":"a4gX5zVDIZdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jVX0P0otIk4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9bMsBeqV8GCf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for epoch in range(n_epochs):\n","    dataloader = DataLoader(dataset, \n","                            batch_size, \n","                            shuffle=True, \n","                            collate_fn=collate_fn,\n","                            drop_last = True,\n","                            )\n","    for i, batch in enumerate(dataloader):\n","        optim.zero_grad()\n","\n","        predict = model(batch['data'].to(device))\n","        loss = loss_func(predict.view(-1, n_classes),\n","                         batch['target'].to(device).view(-1), \n","                         )\n","        loss.backward()\n","        optim.step()\n","        if i % 100 == 0:\n","            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n","   \n","    torch.save(model.state_dict(), f'./rnn_chkpt_{epoch}.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2f3MATJ8GKb","executionInfo":{"status":"ok","timestamp":1666029788753,"user_tz":-180,"elapsed":1283367,"user":{"displayName":"Boris Zhestkov","userId":"15589718157134474454"}},"outputId":"cbc91671-0535-480f-cf89-ea757c624e7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, step: 0, loss: 2.925220489501953\n","epoch: 0, step: 100, loss: 0.3129104673862457\n","epoch: 0, step: 200, loss: 0.17702297866344452\n","epoch: 1, step: 0, loss: 0.19092483818531036\n","epoch: 1, step: 100, loss: 0.15849949419498444\n","epoch: 1, step: 200, loss: 0.19675496220588684\n","epoch: 2, step: 0, loss: 0.1275748461484909\n","epoch: 2, step: 100, loss: 0.08365113288164139\n","epoch: 2, step: 200, loss: 0.09486833214759827\n","epoch: 3, step: 0, loss: 0.07672867178916931\n","epoch: 3, step: 100, loss: 0.07737841457128525\n","epoch: 3, step: 200, loss: 0.07319189608097076\n","epoch: 4, step: 0, loss: 0.0699860230088234\n","epoch: 4, step: 100, loss: 0.0673961415886879\n","epoch: 4, step: 200, loss: 0.06435465812683105\n","epoch: 5, step: 0, loss: 0.04708215594291687\n","epoch: 5, step: 100, loss: 0.05166931077837944\n","epoch: 5, step: 200, loss: 0.041683319956064224\n","epoch: 6, step: 0, loss: 0.015623084269464016\n","epoch: 6, step: 100, loss: 0.03799762204289436\n","epoch: 6, step: 200, loss: 0.02635781280696392\n","epoch: 7, step: 0, loss: 0.02603883109986782\n","epoch: 7, step: 100, loss: 0.04194776713848114\n","epoch: 7, step: 200, loss: 0.03463588282465935\n","epoch: 8, step: 0, loss: 0.03449860215187073\n","epoch: 8, step: 100, loss: 0.022475510835647583\n","epoch: 8, step: 200, loss: 0.034967418760061264\n","epoch: 9, step: 0, loss: 0.017943061888217926\n","epoch: 9, step: 100, loss: 0.02831675298511982\n","epoch: 9, step: 200, loss: 0.019956247881054878\n"]}]},{"cell_type":"code","source":["\n","#example\n","phrase = 'He ran quickly after the red bus and caught it'\n","words = phrase.split(' ')\n","tokens = [dataset.word_vocab[w] for w in words]\n","\n","start = datetime.datetime.now()\n","with torch.no_grad():\n","    model.eval()\n","    predict = model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n","    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n","    end = datetime.datetime.now() - start\n","\n","target_labels = list(dataset.target_vocab.keys())\n","print([target_labels[l-1] for l in labels])"],"metadata":{"id":"9CljFAzIMMEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666028388211,"user_tz":-180,"elapsed":573,"user":{"displayName":"Boris Zhestkov","userId":"15589718157134474454"}},"outputId":"ac850afb-506a-4938-fd9c-4f92fb1fbb9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['PRON', 'VERB', 'ADV', 'SCONJ', 'DET', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'PRON']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"soes4kIU8FDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9PbgCjN48FRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"74gggSX58Fe9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"-57Jq-CW8NmD"}}]}